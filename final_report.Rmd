---
title: '**DC5: Final Report**'
author: "Marium Tapal, Eleni Partakki, Lauren Low, Elisabeth Nesmith"
date: "20 April 2021"
output: 
  html_document:
    code_folding: hide
    theme: sandstone
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE
)
```

```{r packages, message=FALSE}
library(tidyverse)
library(readxl)
library(janitor)
library(here)
library(lubridate)
library(plotly)
library(mosaic)
library(datapasta)
library(patchwork)
```

```{r load-data, message=FALSE}
social_media <- read_csv("~/Downloads/DC5-Data/Y*Int Social Media Data/YInt.csv")
static_sensor_locations <- read_csv("~/Downloads/DC5-Data/Sensor Data and Maps/StaticSensorLocations.csv")
static_sensor_readings <- read_csv("~/Downloads/DC5-Data/Sensor Data and Maps/StaticSensorReadings.csv")
mobile_sensor_readings <- read_csv("~/Downloads/DC5-Data/Sensor Data and Maps/MobileSensorReadings.csv")
mc1_reports_data <- read_csv("~/Downloads/DC5-Data/Damage Reports/mc1-reports-data.csv") %>%
  mutate(location = as.factor(location))
```


## Generate a master timeline of events and trends during the emergency response. Pay particular attention to places where the timing of events is uncertain, and note which data underlies that uncertainty.

## Emergency responders will base their initial response on the earthquake shake map, but their response may change based on damage reports from citizens on the ground. How would you prioritize neighborhoods for response?

```{r}
# change data to long format for heat map
mc1_reports_data_long <- reshape2::melt(mc1_reports_data, id.vars = c("time", "location"))

# handling missing and repeated data
heat_map_data <- mc1_reports_data_long %>%
  group_by(time, location, variable) %>%
  mutate(value = round(mean(value, na.rm = TRUE), 2)) %>%
  unique()

# by location
shake_data <- heat_map_data %>% 
  filter(variable == "shake_intensity")

ggplot(shake_data, aes(time, location)) +
  geom_tile(aes(fill = value)) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(
    title = "Shake Intensity Over Time by Location",
    x = "Time", y = "Location", fill = " Value"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Based on the chart above of the shake intensity reported over time in the 19 St. Himark locations, the most red locations (indicative of higher damage) should receive help first. Locations 4, 7, 12, and 18 should receive aid,

## Compare the reliability of neighborhood reports. Which neighborhoods are providing reliable reports? Provide a rationale for your response.

```{r}
# by location
ggplot(heat_map_data, aes(time, variable)) +
  geom_tile(aes(fill = value)) +
  facet_wrap(~location) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "Report Variables Over Time by Location", x = "Time", y = "Report Variable", fill = " Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

In this plot above, we see the reports of all 19 locations over the 5 day period. It is apparent that some locations have less reliable reporting. For example, location 4, 7, 10, and 17 have sparse maps indicating that there was missing data. 

Similarly, we can see that locations 2, 6, and 15 have the most regular and reliable reporting.

In making this plot, we realized that there was missing and corrupt report data in many locations. We dealt with this by: 

- **missing data:** we dropped the values
- **corrupt data:** we defined this as the repeated values for the same location and variable at the same time. We took the average of the multiple reports of the same reading.

## Compare radiation measurements over time from both static and mobile sensors to identify areas where elevated radiation is detected. How does this change over time? How should the risk of radiation damage be mitigated?

## Characterize conditions across the city, and recommend how resources should be allocated at 5 hours and 30 hours after the earthquake. Include evidence from the data to support these recommendations. Consider how to allocate resources such as road crews, sewer repair crews, power, and rescue teams.

```{r}
time_data <- heat_map_data %>% 
  filter(time >= "2020-04-08 09:35:00" & time <= "2020-04-08 10:35:00")

ggplot(time_data, aes(time, location)) +
  geom_tile(aes(fill = value)) +
  facet_wrap(~variable) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(
    title = "Location Over Time by Report Variables",
    x = "Time", y = "Location", fill = " Value"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

We guess that the major quake happened around 8:35 am on April 8th. 5 hours and 30 minutes later is approximately 2:05 pm on the same day. The above plot shows the reporting from 2:05 pm $\pm$ 30 minutes. 

According to this chart, and excluding the missing data, it looks like power is the most affected, followed by roads and bridges, sewer and water and buildings. Therefore, the most resources should be allocated to power and the rest to road crews, rescue teams and sewer repair crews!

## Identify any times when conditions change in a way that warrants a re-allocation of city resources. What were the conditions before and after the inflection point? What locations were affected? Which resources are involved?

- pre shock
- major quake
- after shock

## Take the "pulse"" of the community. How has the earthquake affected life in St. Himark? What is the community experiencing?

## Are there instances where a pattern emerges in one set of data before it presents itself in another? Could one data stream be used to predict events in the others? Provide examples you identify.

## The data in this challenge can be analyzed either as a static collection or as a dynamic stream of data, as it would occur in a real emergency. Can you find a way to bring your analysis online to fuse multiple data streams together as events are unfolding?

In this challenge we analyzed data as a static collection. However, in a real emergency, a dynamic stream of data would be useful. As data continuously flows in, some tools like a dashboard, could be helpful in flagging respective authorities if particular things need attention. For example, in our analysis above, we indicate that after the major quake almost all the report variables have an increased rating. Power and medical resources need attention so the system would alert the power companies and hospitals!
